run_title: ''

training_model:
  kind: default
  visualize_each_iters: 1 #1000
  concat_mask: true
  store_discr_outputs_for_vis: true

losses:
  l1:
    weight_missing: 0
    weight_known: 10
  perceptual:
    weight: 0
  adversarial:
    kind: r1
    weight: 10
    gp_coef: 0.001
    mask_as_fake_target: true
    allow_scale_mask: true
  feature_matching:
    weight: 100
  resnet_pl:
    weight: 30
    weights_path: ${env:TORCH_HOME}

generator:
  kind: ffc_resnet
  input_nc: 4
  output_nc: 3
  ngf: 64
  n_downsampling: 3
  n_blocks: 18
  add_out_act: sigmoid
  init_conv_kwargs:
    ratio_gin: 0
    ratio_gout: 0
    enable_lfu: false
  downsample_conv_kwargs:
    ratio_gin: ${generator.init_conv_kwargs.ratio_gout}
    ratio_gout: ${generator.downsample_conv_kwargs.ratio_gin}
    enable_lfu: false
  resnet_conv_kwargs:
    ratio_gin: 0.75
    ratio_gout: ${generator.resnet_conv_kwargs.ratio_gin}
    enable_lfu: false

data:
  batch_size: 2
  val_batch_size: 2
  num_workers: 3

  train:
    indir: ${location.data_root_dir}/train
    out_size: 256
    mask_gen_kwargs:  # probabilities do not need to sum to 1, they are re-normalized in mask generator
      irregular_proba: 1
      irregular_kwargs:
        max_angle: 4
        max_len: 200
        max_width: 100
        max_times: 5
        min_times: 1

      box_proba: 1
      box_kwargs:
        margin: 10
        bbox_min_size: 30
        bbox_max_size: 150
        max_times: 4
        min_times: 1

      segm_proba: 0

    transform_variant: distortions
    dataloader_kwargs:
      batch_size: ${data.batch_size}
      shuffle: True
      num_workers: ${data.num_workers}

  val:
    indir: ${location.data_root_dir}/val
    img_suffix: .jpg
    dataloader_kwargs:
      batch_size: ${data.val_batch_size}
      shuffle: False
      num_workers: ${data.num_workers}

location:
  data_root_dir: /home/andre/Desktop/lama-new/dataTrain
  out_root_dir: /home/andre/Desktop/lama-new/experiments
  tb_dir: /home/andre/Desktop/lama-new/tb_logs
  pretrained_models: /home/andre/Desktop/lama-new

discriminator:
  kind: pix2pixhd_nlayer
  input_nc: 3
  ndf: 64
  n_layers: 4

optimizers:
  generator:
    kind: adam
    lr: 0.001
  discriminator:
    kind: adam
    lr: 0.0001

visualizer:
  kind: directory
  outdir: samples
  key_order:
    - image
    - predicted_image
    - discr_output_fake
    - discr_output_real
    - inpainted
  rescale_keys:
    - discr_output_fake
    - discr_output_real

evaluator:
  kind: default
  inpainted_key: inpainted  # if you want to evaluate before blending with original image by mask, set predicted_image
  integral_kind: ssim_fid100_f1

trainer:
  kwargs:
    gpus: -1
    accelerator: ddp
    max_epochs: 2
    gradient_clip_val: 1
    log_gpu_memory: None  # set to min_max or all for debug
    limit_train_batches: 1 #25000
    val_check_interval: ${trainer.kwargs.limit_train_batches}
    # fast_dev_run: True  # uncomment for faster debug
    # track_grad_norm: 2  # uncomment to track L2 gradients norm
    log_every_n_steps: 250
    precision: 32
  #  precision: 16
  #  amp_backend: native
  #  amp_level: O1
    # resume_from_checkpoint: path  # override via command line trainer.resume_from_checkpoint=path_to_checkpoint
    terminate_on_nan: False
    # auto_scale_batch_size: True  # uncomment to find largest batch size
    check_val_every_n_epoch: 1
    num_sanity_val_steps: 8
  #  limit_val_batches: 1000000
    replace_sampler_ddp: False

  checkpoint_kwargs:
    verbose: True
    save_top_k: 5
    save_last: True
    period: 1
    monitor: val_ssim_fid100_f1_total_mean
    mode: max


defaults:
  - hydra: overrides
